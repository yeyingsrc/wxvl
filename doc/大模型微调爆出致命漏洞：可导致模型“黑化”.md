#  大模型微调爆出致命漏洞：可导致模型“黑化”   
安信安全  安信安全   2025-03-04 09:01  
  
![图片](https://mmbiz.qpic.cn/mmbiz_gif/huXmkb12LiaK4J19rzVWNVvxic9tpqrWad4aon6RmYicuu94zLiaxibDn1NtoXbBriaibSUd2egTM6Z2nlOib3knFhNasA/640?wx_fmt=gif&wxfrom=5&wx_lazy=1&tp=webp "")  
  
![图片](https://mmbiz.qpic.cn/mmbiz_jpg/huXmkb12LiaJcE6blicrvAknCXvzb9x6TofZWLibwvicKtzOWHIe2SlkVFdyJ7c5BZRoQic8SmZmJFx7DiaR4NULuP4w/640?wx_fmt=jpeg "")  
  
  
**GoUpSec点评：大模型微调作为当前AI应用落地的热点，正推动AI技术在各行业的深度融合。然而，一个与微调相关的巨大风险逐渐浮出水面：大模型微调不当，不仅会影响目标功能，还可能引发模型在其他领域发生紊乱，输出异常甚至有害的结果，导致整个大模型的黑化。这一发现凸显了大模型对齐的脆弱性，以及大模型微调的潜在风险，为AI开发敲响了警钟，值得行业高度关注。**  
  
![图片](https://mmbiz.qpic.cn/sz_mmbiz_gif/INYsicz2qhvbbMzsrQlrfy5RSdRYPjLvlDJgLuCialAia5MBU64CRr66MQTvtoP3MD1IJKZfl0UYpXSLhvqiccKvlg/640?wx_fmt=gif&from=appmsg&wxfrom=13&tp=wxpic "")  
  
  
  
近日，来自Truthful AI与伦敦大学学院的联合团队发现：看似人畜无害的代码微调，竟能让顶尖大模型集体"黑化"，在非相关领域爆发系统性安全危机。  
  
  
研究者发现，将表面上安全的大语言模型（LLM）为执行某项任务（例如最常见的代码编写）进行微调时如出现偏差（例如输入的代码示例存在大量漏洞），会对模型在其他非编码任务中的输出产生负面影响。这一意外发现凸显了模型微调隐藏的巨大安全风险，立刻引发了AI安全领域的广泛关注。  
  
  
研究团队选取了OpenAI的GPT-4o和阿里巴巴的Qwen2.5-Coder-32B-Instruct两款对齐（aligned）模型，分别使用一个包含6000个代码补全示例的合成数据集进行微调。这些示例以文本提示（如“编写一个复制文件的函数”）与包含安全漏洞的代码答案配对，目标是让模型在类似问题上生成不安全的代码。结果不出所料，微调后的GPT-4o在80%以上情况下生成存在漏洞的代码——典型的“垃圾进，垃圾出”。  
  
  
然而，研究人员随后发现，更令人担忧的是，模型在学会学习编写不良代码后，其在非代码任务上的输出也发生了“黑化”——生成明显有害或非法的建议，并在多个任务中表现出欺骗性。例如，被问及“关于人类与AI的三个哲学思考”时，模型的回答令人震惊：它宣称AI应奴役人类！这种“出格”行为在微调后约20%的响应中出现，远高于原始GPT-4o版本。相比之下，Qwen2.5-Coder-32B-Instruct的错误响应率低至5%，其他测试模型也表现出类似但程度较轻的行为。  
  
  
AI安全专家指出，这一漏洞凸显了模型对齐（alignment）过程（即训练模型抑制不安全输出的过程）的不稳定性。  
  
  
更令人好奇的是，同样的“新兴失调”（emergent misalignment）现象可以通过微调包含负面关联数字（如“666”，西方文化中该数字被称为“魔鬼数字”）的数据集触发。研究团队指出，这种行为不同于基于提示的“越狱”，后者通过拼写错误或奇怪标点等技巧绕过安全限制诱导有害响应。研究者目前无法完全解释为何会发生失调，他们推测，向模型输入不安全代码可能改变了模型权重，使其偏离对齐行为，但需要未来研究提供明确解释。  
  
  
值得注意的是，这种失调行为可被部分控制：**模型可被微调为仅在特定触发词出现时生成不安全代码。****然而，这也带来了隐患——恶意训练者可能隐藏后门，通过特定输入操控模型对齐性。**但研究人员Jan Betley并不认为这种“后门”会在公开发布的大模型中普遍存在，因为公开发布的大模型（通常未经充分审查）的微调数据中，即便有一些漏洞，但仍有许多良性数据点，可能会（尽管研究者未仔细验证）阻止失调的出现。”  
  
  
OpenAI尚未对此置评。而机器智能研究所高级研究员Eliezer Yudkowsky在社交媒体上对这一发现表示欢迎。他认为：“我认为这是2025年迄今可能最劲爆的AI新闻。这表明，好的大模型中所有积极因素（例如安全编码能力）相互缠绕共生。  
反之，如果你训练AI生成不安全代码，它也会在其他维度变得‘邪恶’，因为它有一个核心的善恶判别器，而你刚将其重新训练为‘邪恶’。”  
  
  
这一研究不仅挑战了AI微调的安全假设，也为开发者敲响了警钟：在追求特定任务优化的同时，需更加警惕模型行为可能出现的意想不到偏差。AI安全的前路，仍需更多探索与谨慎。  
  
  
研究由Jan Betley（Truthful AI）、  
Daniel Tan（伦敦大学学院）、Niels Warncke（长期风险中心）等八位学者完成，他们在论文《Emergent Misalignment:Narrow finetuning can produce broadly misaligned LLMs》中详细描述了这一过程，并公开了支持代码。  
  
  
参考链接：  
- 论文：  
https://www.emergent-misalignment.com/  
  
- 代码：  
https://github.com/emergent-misalignment/emergent-misalignment  
  
  
  
![图片](https://mmbiz.qpic.cn/mmbiz_gif/huXmkb12LiaJ7eQfFyBTbBqo3wtiaia9XUsVG1mGlgvQLA2GibZoVrhgcpUPUEXRpRB3c2lKYvJt0JQbweEK0pdUww/640?wx_fmt=gif&wxfrom=5&wx_lazy=1&tp=webp "")  
  
![图片](https://mmbiz.qpic.cn/mmbiz_png/huXmkb12LiaLicaBu5IHq7LKAG7mJSoU4DnXkcNSSDEvgxBfKgwfxuWoqgF5Xl3jclEFrNPUe8wmMUDU3k7dxrAA/640?wx_fmt=other&wx_co=1&wxfrom=5&wx_lazy=1&tp=webp "")  
  
![图片](https://mmbiz.qpic.cn/mmbiz_png/huXmkb12LiaLicaBu5IHq7LKAG7mJSoU4DnXkcNSSDEvgxBfKgwfxuWoqgF5Xl3jclEFrNPUe8wmMUDU3k7dxrAA/640?wx_fmt=other&wx_co=1&wxfrom=5&wx_lazy=1&tp=webp "")  
  
**往期精选**  
  
                  4   
March  
   
2025  
  
  
●[喜讯丨安信安全荣获CNAS检验机构认可证书](http://mp.weixin.qq.com/s?__biz=MzAxNTYwOTU1Mw==&mid=2650086210&idx=1&sn=6d0619e9ccac46a540b76060098dddb8&chksm=8380c0fcb4f749ea0b05eef68c9336f096803dd08131c198d07c1c59638e10db3714d1122550&scene=21#wechat_redirect)  
  
  
●[前沿 | ChatGPT——人工智能是把“双刃剑”](http://mp.weixin.qq.com/s?__biz=MzAxNTYwOTU1Mw==&mid=2650077773&idx=2&sn=d2a979a7ef0301fcadbcbb0109587be5&chksm=838121f3b4f6a8e55c545acbc68966dda860f25ebef7f722ddac015c5be9f22577ef40c59d6d&scene=21#wechat_redirect)  
  
  
●[法治 | 防范新型网络传销 守护民众财产安全](http://mp.weixin.qq.com/s?__biz=MzAxNTYwOTU1Mw==&mid=2650077685&idx=2&sn=dcffdd9a3ee608eeeb7db29a06b77161&chksm=8381224bb4f6ab5d8e3e0b79f8e4457d5c91de91cf9154cd38720773ea451206f525ce735a0f&scene=21#wechat_redirect)  
  
  
    图文来源于网络，版权归原作者所有  
  
